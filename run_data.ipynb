{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "# import mlflow\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import inspect\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import copy\n",
    "import re\n",
    "import warnings\n",
    "import torch\n",
    "import tensorflow as tf\n",
    "import scipy.signal as sn\n",
    "import argparse\n",
    "from types import SimpleNamespace\n",
    "from pprint import pprint\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.rcParams['figure.figsize'] = (16, 3)\n",
    "# pd.reset_option('vdisplay.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'exp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexp_main\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Exp_Main\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'exp'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import torch\n",
    "from exp.exp_main import Exp_Main\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['--is_training', '1', '--root_path', './dataset/', '--data_path', 'electricity.csv', '--model_id', 'electricity_96_96', '--model', 'Autoformer', '--data', 'custom', '--features', 'M', '--seq_len', '96', '--label_len', '48', '--pred_len', '96', '--e_layers', '2', '--d_layers', '1', '--factor', '3', '--enc_in', '321', '--dec_in', '321', '--c_out', '321', '--des', \"'Exp'\", '--itr', '1']\n"
     ]
    }
   ],
   "source": [
    "command = f\"python -u run_longExp.py \\\n",
    "      --is_training 1 \\\n",
    "      --root_path ./dataset/ \\\n",
    "      --data_path electricity.csv \\\n",
    "      --model_id electricity_96_$pred_len \\\n",
    "      --model $model_name \\\n",
    "      --data custom \\\n",
    "      --features M \\\n",
    "      --seq_len 96 \\\n",
    "      --label_len 48 \\\n",
    "      --pred_len $pred_len \\\n",
    "      --e_layers 2 \\\n",
    "      --d_layers 1 \\\n",
    "      --factor 3 \\\n",
    "      --enc_in 321 \\\n",
    "      --dec_in 321 \\\n",
    "      --c_out 321 \\\n",
    "      --des 'Exp' \\\n",
    "      --itr 1 >logs/LongForecasting/$model_name'_electricity_'$pred_len.log\"\n",
    "\n",
    "\n",
    "def get_variables_from_command(command: str):\n",
    "    args = {}\n",
    "    lines = command.split(\"--\")\n",
    "    lis = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"python\"):\n",
    "            continue\n",
    "        # print(line)\n",
    "        temp = line.split(\" \")\n",
    "        # if temp[1]\n",
    "\n",
    "        args[temp[0]] = temp[1]\n",
    "        temp[0] = \"--\" + temp[0]\n",
    "        lis.extend(temp)\n",
    "\n",
    "    print(lis)\n",
    "    args = SimpleNamespace(**args)\n",
    "    return args, lis\n",
    "\n",
    "\n",
    "replacement = dict(\n",
    "    pred_len=str(96),\n",
    "    model_name=\"Autoformer\",\n",
    ")\n",
    "\n",
    "for k, v in replacement.items():\n",
    "    k = \"$\" + k\n",
    "    command = command.replace(k, v)\n",
    "\n",
    "commands = command.split(\">\")\n",
    "command = commands[0]\n",
    "log_path = commands[1]\n",
    "\n",
    "\n",
    "args, lis = get_variables_from_command(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "\n",
    "def parse_args(lis):\n",
    "    fix_seed = 2021\n",
    "    random.seed(fix_seed)\n",
    "    torch.manual_seed(fix_seed)\n",
    "    np.random.seed(fix_seed)\n",
    "\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description=\"Autoformer & Transformer family for Time Series Forecasting\"\n",
    "    )\n",
    "\n",
    "    # basic config\n",
    "    parser.add_argument(\n",
    "        \"--is_training\", type=int, required=True, default=1, help=\"status\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_only\",\n",
    "        type=bool,\n",
    "        required=False,\n",
    "        default=False,\n",
    "        help=\"perform training on full input dataset without validation and testing\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model_id\", type=str, required=True, default=\"test\", help=\"model id\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--model\",\n",
    "        type=str,\n",
    "        required=True,\n",
    "        default=\"Autoformer\",\n",
    "        help=\"model name, options: [Autoformer, Informer, Transformer]\",\n",
    "    )\n",
    "\n",
    "    # data loader\n",
    "    parser.add_argument(\n",
    "        \"--data\", type=str, required=True, default=\"ETTm1\", help=\"dataset type\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--root_path\",\n",
    "        type=str,\n",
    "        default=\"./data/ETT/\",\n",
    "        help=\"root path of the data file\",\n",
    "    )\n",
    "    parser.add_argument(\"--data_path\", type=str, default=\"ETTh1.csv\", help=\"data file\")\n",
    "    parser.add_argument(\n",
    "        \"--features\",\n",
    "        type=str,\n",
    "        default=\"M\",\n",
    "        help=\"forecasting task, options:[M, S, MS]; M:multivariate predict multivariate, S:univariate predict univariate, MS:multivariate predict univariate\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--target\", type=str, default=\"OT\", help=\"target feature in S or MS task\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--freq\",\n",
    "        type=str,\n",
    "        default=\"h\",\n",
    "        help=\"freq for time features encoding, options:[s:secondly, t:minutely, h:hourly, d:daily, b:business days, w:weekly, m:monthly], you can also use more detailed freq like 15min or 3h\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--checkpoints\",\n",
    "        type=str,\n",
    "        default=\"./checkpoints/\",\n",
    "        help=\"location of model checkpoints\",\n",
    "    )\n",
    "\n",
    "    # forecasting task\n",
    "    parser.add_argument(\"--seq_len\", type=int, default=96, help=\"input sequence length\")\n",
    "    parser.add_argument(\"--label_len\", type=int, default=48, help=\"start token length\")\n",
    "    parser.add_argument(\n",
    "        \"--pred_len\", type=int, default=96, help=\"prediction sequence length\"\n",
    "    )\n",
    "\n",
    "    # DLinear\n",
    "    parser.add_argument(\n",
    "        \"--individual\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"DLinear: a linear layer for each variate(channel) individually\",\n",
    "    )\n",
    "    # Formers\n",
    "    parser.add_argument(\n",
    "        \"--embed_type\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"0: default 1: value embedding + temporal embedding + positional embedding 2: value embedding + temporal embedding 3: value embedding + positional embedding 4: value embedding\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--enc_in\", type=int, default=7, help=\"encoder input size\"\n",
    "    )  # DLinear with --individual, use this hyperparameter as the number of channels\n",
    "    parser.add_argument(\"--dec_in\", type=int, default=7, help=\"decoder input size\")\n",
    "    parser.add_argument(\"--c_out\", type=int, default=7, help=\"output size\")\n",
    "    parser.add_argument(\"--d_model\", type=int, default=512, help=\"dimension of model\")\n",
    "    parser.add_argument(\"--n_heads\", type=int, default=8, help=\"num of heads\")\n",
    "    parser.add_argument(\"--e_layers\", type=int, default=2, help=\"num of encoder layers\")\n",
    "    parser.add_argument(\"--d_layers\", type=int, default=1, help=\"num of decoder layers\")\n",
    "    parser.add_argument(\"--d_ff\", type=int, default=2048, help=\"dimension of fcn\")\n",
    "    parser.add_argument(\n",
    "        \"--moving_avg\", type=int, default=25, help=\"window size of moving average\"\n",
    "    )\n",
    "    parser.add_argument(\"--factor\", type=int, default=1, help=\"attn factor\")\n",
    "    parser.add_argument(\n",
    "        \"--distil\",\n",
    "        action=\"store_false\",\n",
    "        help=\"whether to use distilling in encoder, using this argument means not using distilling\",\n",
    "        default=True,\n",
    "    )\n",
    "    parser.add_argument(\"--dropout\", type=float, default=0.05, help=\"dropout\")\n",
    "    parser.add_argument(\n",
    "        \"--embed\",\n",
    "        type=str,\n",
    "        default=\"timeF\",\n",
    "        help=\"time features encoding, options:[timeF, fixed, learned]\",\n",
    "    )\n",
    "    parser.add_argument(\"--activation\", type=str, default=\"gelu\", help=\"activation\")\n",
    "    parser.add_argument(\n",
    "        \"--output_attention\",\n",
    "        action=\"store_true\",\n",
    "        help=\"whether to output attention in ecoder\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--do_predict\",\n",
    "        action=\"store_true\",\n",
    "        help=\"whether to predict unseen future data\",\n",
    "    )\n",
    "\n",
    "    # optimization\n",
    "    parser.add_argument(\n",
    "        \"--num_workers\", type=int, default=10, help=\"data loader num workers\"\n",
    "    )\n",
    "    parser.add_argument(\"--itr\", type=int, default=2, help=\"experiments times\")\n",
    "    parser.add_argument(\"--train_epochs\", type=int, default=10, help=\"train epochs\")\n",
    "    parser.add_argument(\n",
    "        \"--batch_size\", type=int, default=32, help=\"batch size of train input data\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--patience\", type=int, default=3, help=\"early stopping patience\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\", type=float, default=0.0001, help=\"optimizer learning rate\"\n",
    "    )\n",
    "    parser.add_argument(\"--des\", type=str, default=\"test\", help=\"exp description\")\n",
    "    parser.add_argument(\"--loss\", type=str, default=\"mse\", help=\"loss function\")\n",
    "    parser.add_argument(\n",
    "        \"--lradj\", type=str, default=\"type1\", help=\"adjust learning rate\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_amp\",\n",
    "        action=\"store_true\",\n",
    "        help=\"use automatic mixed precision training\",\n",
    "        default=False,\n",
    "    )\n",
    "\n",
    "    # GPU\n",
    "    parser.add_argument(\"--use_gpu\", type=bool, default=True, help=\"use gpu\")\n",
    "    parser.add_argument(\"--gpu\", type=int, default=0, help=\"gpu\")\n",
    "    parser.add_argument(\n",
    "        \"--use_multi_gpu\", action=\"store_true\", help=\"use multiple gpus\", default=False\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--devices\", type=str, default=\"0,1,2,3\", help=\"device ids of multile gpus\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--test_flop\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"See utils/tools for usage\",\n",
    "    )\n",
    "\n",
    "    args = parser.parse_args(lis)\n",
    "\n",
    "    args.use_gpu = True if torch.cuda.is_available() and args.use_gpu else False\n",
    "\n",
    "    if args.use_gpu and args.use_multi_gpu:\n",
    "        args.dvices = args.devices.replace(\" \", \"\")\n",
    "        device_ids = args.devices.split(\",\")\n",
    "        args.device_ids = [int(id_) for id_ in device_ids]\n",
    "        args.gpu = args.device_ids[0]\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "args = parse_args(lis)\n",
    "args.train_epochs = 1\n",
    "args.d_ff = 128\n",
    "args.d_model = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use CPU\n",
      ">>>>>>>start training : electricity_96_96_Autoformer_custom_ftM_sl96_ll48_pl96_dm128_nh8_el2_dl1_df128_fc3_ebtimeF_dtTrue_'Exp'_0>>>>>>>>>>>>>>>>>>>>>>>>>>\n",
      "train 18221\n",
      "val 2537\n",
      "test 5165\n",
      "\titers: 100, epoch: 1 | loss: 0.3922830\n",
      "\tspeed: 0.6205s/iter; left time: 291.6303s\n"
     ]
    }
   ],
   "source": [
    "Exp = Exp_Main\n",
    "\n",
    "if args.is_training:\n",
    "    for ii in range(args.itr):\n",
    "        # setting record of experiments\n",
    "        setting = \"{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}\".format(\n",
    "            args.model_id,\n",
    "            args.model,\n",
    "            args.data,\n",
    "            args.features,\n",
    "            args.seq_len,\n",
    "            args.label_len,\n",
    "            args.pred_len,\n",
    "            args.d_model,\n",
    "            args.n_heads,\n",
    "            args.e_layers,\n",
    "            args.d_layers,\n",
    "            args.d_ff,\n",
    "            args.factor,\n",
    "            args.embed,\n",
    "            args.distil,\n",
    "            args.des,\n",
    "            ii,\n",
    "        )\n",
    "\n",
    "        exp = Exp(args)  # set experiments\n",
    "        print(\">>>>>>>start training : {}>>>>>>>>>>>>>>>>>>>>>>>>>>\".format(setting))\n",
    "        exp.train(setting)\n",
    "\n",
    "        if not args.train_only:\n",
    "            print(\n",
    "                \">>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\".format(setting)\n",
    "            )\n",
    "            exp.test(setting)\n",
    "\n",
    "        if args.do_predict:\n",
    "            print(\n",
    "                \">>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\".format(\n",
    "                    setting\n",
    "                )\n",
    "            )\n",
    "            exp.predict(setting, True)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "else:\n",
    "    ii = 0\n",
    "    setting = \"{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_dt{}_{}_{}\".format(\n",
    "        args.model_id,\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        args.embed,\n",
    "        args.distil,\n",
    "        args.des,\n",
    "        ii,\n",
    "    )\n",
    "\n",
    "    exp = Exp(args)  # set experiments\n",
    "\n",
    "    if args.do_predict:\n",
    "        print(\">>>>>>>predicting : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\".format(setting))\n",
    "        exp.predict(setting, True)\n",
    "    else:\n",
    "        print(\">>>>>>>testing : {}<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\".format(setting))\n",
    "        exp.test(setting, test=1)\n",
    "\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
